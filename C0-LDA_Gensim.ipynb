{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Topic Model (LDA) on the Airbnb Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# import and setup modules we'll be using in this notebook\n",
    "import logging\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import sys\n",
    "reload(sys)\n",
    "import cPickle\n",
    "import re\n",
    "sys.setdefaultencoding('utf8')\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.utils import smart_open, simple_preprocess\n",
    "from gensim.corpora.wikicorpus import _extract_pages, filter_wiki\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token.lower() not in STOPWORDS]\n",
    "\n",
    "def iter_token(id_,comment):\n",
    "    \"\"\"Yield each article from the Wikipedia dump, as a `(title, tokens)` 2-tuple.\"\"\"\n",
    "  #  ignore_namespaces = 'Wikipedia Category File Portal Template MediaWiki User Help Book Draft'.split()\n",
    "    for id_,comment in zip(id_,comment):\n",
    "        tokens=tokenize(comment)\n",
    "       # if len(tokens) < 50 :\n",
    "        #    continue  # ignore short articles and various meta-articles\n",
    "        yield id_, tokens\n",
    "def person_remover(comment,person):\n",
    "    return comment.replace(person,'')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27247"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_to_read = ['listing_id','id','lan']\n",
    "df_all_reviews= pd.read_csv('../DataFiles/merged_reviews_withCitiesLan.csv',usecols=col_to_read)\n",
    "\n",
    "df_person = pd.read_csv('../DataFiles/sentences_withPersonName_NYC_.csv',header=None,sep='\\t')\n",
    "df_person.columns=['no','year','city','id','person','comment']\n",
    "\n",
    "df_person=df_person.dropna()\n",
    "df_person.year=df_person.year.astype(int)\n",
    "df_person.id=df_person.id.astype(int)\n",
    "df=pd.merge(df_person,df_all_reviews,on='id',how='left')\n",
    "df=df[df.lan=='en']\n",
    "df.listing_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['comment'] = df.apply(lambda row: person_remover(row['comment'], row['person']), axis=1)\n",
    "\n",
    "df_text=df.groupby(['listing_id','year'])['comment'].apply(lambda x: '.'.join(x)).reset_index()\n",
    "df_text=df_text.sort_values(['listing_id','year'], ascending=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_text=df_text[df_text.year>2009]\n",
    "#df_=pd.DataFrame(df_text.groupby(['listing_id']).year.count())\n",
    "#indices=df_[df_.year==7].index.tolist()\n",
    "#df_text=df_text[df_text['listing_id'].isin(indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_text.to_csv('../DataFiles/sentences_withPersonName_NYC_listing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_text=pd.read_csv('../DataFiles/sentences_withPersonName_NYC_listing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "docs = list(df_text.dropna().comment)\n",
    "years= list(df_text.dropna().year)\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "\n",
    "    docs[idx]=re.sub(r'[^\\x00-\\x7F]+',' ', docs[idx])\n",
    "    docs[idx] = docs[idx].lower()# Convert to lowercase.\n",
    "    docs[idx] =tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "#docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "            \n",
    "# Lemmatize all words in documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "dictionary.save('nyc_lda.dic')\n",
    "\n",
    "with open(r\"docs.pickle\", \"wb\") as output_file:\n",
    "    cPickle.dump(docs, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(docs),len(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing documents, and save the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Vectorize data.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "with open(r\"corpus.pickle\", \"wb\") as output_file:\n",
    "    cPickle.dump(corpus, output_file)\n",
    "with open(r\"years.pickle\", \"wb\") as output_file:\n",
    "    cPickle.dump(years, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Save the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)\n",
    "\n",
    "model.save('lda.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High frequent words in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.056*\"exactly\" + 0.039*\"described\" + 0.038*\"clean\" + 0.030*\"bed\" + 0.027*\"towel\" + 0.027*\"picture\" + 0.026*\"kitchen\" + 0.020*\"air\" + 0.018*\"exactly_described\" + 0.018*\"provided\"'),\n",
       " (1,\n",
       "  u'0.057*\"restaurant\" + 0.047*\"recommendation\" + 0.041*\"area\" + 0.034*\"local\" + 0.032*\"gave\" + 0.031*\"bar\" + 0.028*\"eat\" + 0.027*\"information\" + 0.026*\"tip\" + 0.021*\"neighborhood\"'),\n",
       " (2,\n",
       "  u'0.060*\"williamsburg\" + 0.055*\"loft\" + 0.043*\"hidden\" + 0.037*\"view\" + 0.020*\"url_hidden\" + 0.020*\"url\" + 0.019*\"sensitive\" + 0.017*\"art\" + 0.017*\"content\" + 0.016*\"sensitive_content\"'),\n",
       " (3,\n",
       "  u'0.039*\"home\" + 0.035*\"wonderful\" + 0.025*\"family\" + 0.023*\"brooklyn\" + 0.021*\"beautiful\" + 0.020*\"lovely\" + 0.017*\"husband\" + 0.017*\"thank\" + 0.016*\"house\" + 0.012*\"wife\"'),\n",
       " (4,\n",
       "  u'0.043*\"subway\" + 0.036*\"close\" + 0.028*\"location\" + 0.026*\"located\" + 0.021*\"manhattan\" + 0.021*\"minute\" + 0.021*\"walk\" + 0.019*\"away\" + 0.017*\"station\" + 0.016*\"train\"'),\n",
       " (5,\n",
       "  u'0.085*\"recommend\" + 0.065*\"nyc\" + 0.057*\"new\" + 0.051*\"definitely\" + 0.047*\"york\" + 0.047*\"new_york\" + 0.032*\"highly\" + 0.028*\"staying\" + 0.027*\"highly_recommend\" + 0.023*\"trip\"'),\n",
       " (6,\n",
       "  u'0.024*\"meet\" + 0.023*\"arrived\" + 0.022*\"met\" + 0.018*\"sure\" + 0.017*\"late\" + 0.017*\"day\" + 0.015*\"arrival\" + 0.014*\"needed\" + 0.012*\"got\" + 0.011*\"let\"'),\n",
       " (7,\n",
       "  u'0.055*\"easy\" + 0.041*\"question\" + 0.039*\"helpful\" + 0.037*\"responsive\" + 0.034*\"check\" + 0.029*\"accommodating\" + 0.026*\"communication\" + 0.020*\"quick\" + 0.018*\"perfect\" + 0.017*\"thanks\"'),\n",
       " (8,\n",
       "  u'0.053*\"nice\" + 0.039*\"friendly\" + 0.033*\"helpful\" + 0.027*\"welcoming\" + 0.026*\"time\" + 0.020*\"room\" + 0.020*\"kind\" + 0.019*\"good\" + 0.017*\"comfortable\" + 0.016*\"clean\"'),\n",
       " (9,\n",
       "  u'0.193*\"welcome\" + 0.167*\"feel_welcome\" + 0.164*\"feel\" + 0.053*\"went_way\" + 0.049*\"went\" + 0.048*\"way\" + 0.017*\"david\" + 0.013*\"matt\" + 0.011*\"mile\" + 0.010*\"timely_manner\"')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.LdaModel.load('lda.pkl')\n",
    "#top_topics = model.top_topics(corpus, num_words=5)\n",
    "top_topics=model.show_topics()\n",
    "top_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic0</th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>Topic5</th>\n",
       "      <th>Topic6</th>\n",
       "      <th>Topic7</th>\n",
       "      <th>Topic8</th>\n",
       "      <th>Topic9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"exactly\"</td>\n",
       "      <td>\"recommendation\"</td>\n",
       "      <td>\"art\"</td>\n",
       "      <td>\"brooklyn\"</td>\n",
       "      <td>\"manhattan\"</td>\n",
       "      <td>\"nyc\"</td>\n",
       "      <td>\"sure\"</td>\n",
       "      <td>\"question\"</td>\n",
       "      <td>\"good\"</td>\n",
       "      <td>\"david\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"exactly_described\"</td>\n",
       "      <td>\"information\"</td>\n",
       "      <td>\"url_hidden\"</td>\n",
       "      <td>\"husband\"</td>\n",
       "      <td>\"walk\"</td>\n",
       "      <td>\"new\"</td>\n",
       "      <td>\"arrived\"</td>\n",
       "      <td>\"quick\"</td>\n",
       "      <td>\"helpful\"</td>\n",
       "      <td>\"timely_manner\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"described\"</td>\n",
       "      <td>\"tip\"</td>\n",
       "      <td>\"url\"</td>\n",
       "      <td>\"lovely\"</td>\n",
       "      <td>\"station\"</td>\n",
       "      <td>\"york\"</td>\n",
       "      <td>\"let\"</td>\n",
       "      <td>\"helpful\"</td>\n",
       "      <td>\"comfortable\"</td>\n",
       "      <td>\"went\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"clean\"</td>\n",
       "      <td>\"neighborhood\"</td>\n",
       "      <td>\"loft\"</td>\n",
       "      <td>\"home\"</td>\n",
       "      <td>\"minute\"</td>\n",
       "      <td>\"definitely\"</td>\n",
       "      <td>\"meet\"</td>\n",
       "      <td>\"accommodating\"</td>\n",
       "      <td>\"clean\"</td>\n",
       "      <td>\"matt\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"provided\"</td>\n",
       "      <td>\"gave\"</td>\n",
       "      <td>\"williamsburg\"</td>\n",
       "      <td>\"wonderful\"</td>\n",
       "      <td>\"subway\"</td>\n",
       "      <td>\"recommend\"</td>\n",
       "      <td>\"late\"</td>\n",
       "      <td>\"thanks\"</td>\n",
       "      <td>\"time\"</td>\n",
       "      <td>\"welcome\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"picture\"</td>\n",
       "      <td>\"bar\"</td>\n",
       "      <td>\"sensitive\"</td>\n",
       "      <td>\"thank\"</td>\n",
       "      <td>\"located\"</td>\n",
       "      <td>\"highly\"</td>\n",
       "      <td>\"got\"</td>\n",
       "      <td>\"check\"</td>\n",
       "      <td>\"welcoming\"</td>\n",
       "      <td>\"way\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"towel\"</td>\n",
       "      <td>\"area\"</td>\n",
       "      <td>\"sensitive_content\"</td>\n",
       "      <td>\"family\"</td>\n",
       "      <td>\"away\"</td>\n",
       "      <td>\"new_york\"</td>\n",
       "      <td>\"arrival\"</td>\n",
       "      <td>\"responsive\"</td>\n",
       "      <td>\"room\"</td>\n",
       "      <td>\"feel_welcome\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"air\"</td>\n",
       "      <td>\"local\"</td>\n",
       "      <td>\"view\"</td>\n",
       "      <td>\"wife\"</td>\n",
       "      <td>\"location\"</td>\n",
       "      <td>\"highly_recommend\"</td>\n",
       "      <td>\"day\"</td>\n",
       "      <td>\"easy\"</td>\n",
       "      <td>\"nice\"</td>\n",
       "      <td>\"went_way\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"bed\"</td>\n",
       "      <td>\"restaurant\"</td>\n",
       "      <td>\"hidden\"</td>\n",
       "      <td>\"house\"</td>\n",
       "      <td>\"train\"</td>\n",
       "      <td>\"staying\"</td>\n",
       "      <td>\"needed\"</td>\n",
       "      <td>\"perfect\"</td>\n",
       "      <td>\"kind\"</td>\n",
       "      <td>\"feel\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"kitchen\"</td>\n",
       "      <td>\"eat\"</td>\n",
       "      <td>\"content\"</td>\n",
       "      <td>\"beautiful\"</td>\n",
       "      <td>\"close\"</td>\n",
       "      <td>\"trip\"</td>\n",
       "      <td>\"met\"</td>\n",
       "      <td>\"communication\"</td>\n",
       "      <td>\"friendly\"</td>\n",
       "      <td>\"mile\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Topic0             Topic1               Topic2        Topic3  \\\n",
       "0            \"exactly\"   \"recommendation\"                \"art\"    \"brooklyn\"    \n",
       "1  \"exactly_described\"      \"information\"         \"url_hidden\"     \"husband\"    \n",
       "2          \"described\"              \"tip\"                \"url\"      \"lovely\"    \n",
       "3              \"clean\"      \"neighborhood\"              \"loft\"        \"home\"    \n",
       "4            \"provided\"            \"gave\"       \"williamsburg\"   \"wonderful\"    \n",
       "5            \"picture\"              \"bar\"          \"sensitive\"       \"thank\"    \n",
       "6              \"towel\"             \"area\"   \"sensitive_content\"     \"family\"    \n",
       "7                \"air\"            \"local\"               \"view\"         \"wife\"   \n",
       "8                \"bed\"       \"restaurant\"             \"hidden\"       \"house\"    \n",
       "9            \"kitchen\"              \"eat\"            \"content\"   \"beautiful\"    \n",
       "\n",
       "         Topic4               Topic5      Topic6            Topic7  \\\n",
       "0  \"manhattan\"                \"nyc\"      \"sure\"        \"question\"    \n",
       "1       \"walk\"                \"new\"   \"arrived\"           \"quick\"    \n",
       "2    \"station\"               \"york\"        \"let\"        \"helpful\"    \n",
       "3     \"minute\"         \"definitely\"      \"meet\"   \"accommodating\"    \n",
       "4     \"subway\"          \"recommend\"      \"late\"           \"thanks\"   \n",
       "5    \"located\"             \"highly\"       \"got\"           \"check\"    \n",
       "6       \"away\"           \"new_york\"   \"arrival\"      \"responsive\"    \n",
       "7   \"location\"   \"highly_recommend\"       \"day\"            \"easy\"    \n",
       "8       \"train\"           \"staying\"    \"needed\"         \"perfect\"    \n",
       "9      \"close\"                \"trip\"      \"met\"   \"communication\"    \n",
       "\n",
       "           Topic8           Topic9  \n",
       "0         \"good\"          \"david\"   \n",
       "1      \"helpful\"   \"timely_manner\"  \n",
       "2  \"comfortable\"           \"went\"   \n",
       "3         \"clean\"          \"matt\"   \n",
       "4         \"time\"        \"welcome\"   \n",
       "5    \"welcoming\"            \"way\"   \n",
       "6         \"room\"   \"feel_welcome\"   \n",
       "7         \"nice\"       \"went_way\"   \n",
       "8         \"kind\"           \"feel\"   \n",
       "9     \"friendly\"           \"mile\"   "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.LdaModel.load('lda.pkl')\n",
    "#top_topics = model.top_topics(corpus, num_words=5)\n",
    "top_topics=model.show_topics()\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "#avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "#print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "topics=[]\n",
    "top_topics=model.show_topics()\n",
    "for t in top_topics:\n",
    "    topic={}\n",
    "    for chunk in t[1].split('+'):\n",
    "        topic[chunk.split('*')[1].strip(\"'\")] = float(chunk.split('*')[0])\n",
    "    topics.append(topic)\n",
    "    \n",
    "\n",
    "#pprint(top_topics)\n",
    "df_list=[]\n",
    "\n",
    "for topic in topics:\n",
    "\n",
    "    df_= pd.DataFrame([(str(k),v) for k,v in topic.iteritems() ])\n",
    "    df_.sort_values([1], ascending=False)\n",
    "    \n",
    "    df_list.append( df_[0])\n",
    "\n",
    "df = pd.concat(df_list, axis = 1)\n",
    "df.columns=[ [ 'Topic'+str(i) for i in range(len(df_list))]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.035872025760852314),\n",
       " (1, 0.035589528067941689),\n",
       " (2, 0.014092863641369651),\n",
       " (3, 0.12345158179667924),\n",
       " (4, 0.060668127144712189),\n",
       " (5, 0.083196714904605645),\n",
       " (6, 0.15860428997473705),\n",
       " (7, 0.17155117691777058),\n",
       " (8, 0.30384290796439439),\n",
       " (9, 0.013130783826937388)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_document_topics(corpus[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
